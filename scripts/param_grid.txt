param_grid = {
  'LogisticRegression': {'C': np.logspace(-4, 4, 25),
                        'penalty': ['l1', 'l2', 'elasticnet', 'none'],
                        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],
                        'fit_intercept': [True, False],
                        'intercept_scaling': [0.5, 1, 2],
                        'class_weight': [None, 'balanced']
                        },
  'Ridge' : {'alpha': np.logspace(-5, 5, 75)},
  'DecisionTree': {'ccp_alpha': [0.0] + list(np.logspace(-3, 1, 25)),
                    'class_weight': [None, 'balanced'],
                    'criterion': ['gini', 'entropy', 'log_loss'],
                    'max_depth': [None] + list(range(1, 20)),
                    'max_features': [None, 'auto', 'sqrt', 'log2'],
                    'min_samples_leaf': range(1, 10),
                    'min_samples_split': range(2, 10),
                    'splitter': ['best', 'random']
                    },
  'K-nn': {'n_neighbors': list(range(1, 20, 1)),
            'weights': ['uniform', 'distance'],
            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],
            'p': [1,2]
            },
  'RandomForest': {'ccp_alpha': [0] + list(np.logspace(-3, 1, 10)),
                    'class_weight': [None, 'balanced'],
                    'n_estimators': range(50,500,10),
                    'max_features': [None, 'auto', 'sqrt', 'log2'],
                    'max_depth' : [None,4,6,8,10],
                    'criterion' :['gini', 'entropy']
                    },
  'BernoulliNB': {'alpha': np.logspace(-2, 1, 10),
                'fit_prior': [True, False],
                'class_prior': [None, [0.1,]* len(n_classes)],
                'binarize': [None, -5, -2, 0.0, 2, 5, 10.0]
                },
  'GaussianNB': {'var_smoothing': np.logspace(0,-9, num=20)
                  },
  'NearestCentroid': {'shrink_threshold': np.logspace(0, 1, 20),
                    'metric': ['euclidean', 'manhattan']
                    },
  'SVC': {'C': np.logspace(-4, 4, 10),
          'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
          'degree': range(2,5),
          'gamma': np.logspace(-3, 1, 10)},
  'LabelPropagation': {'n_neighbors': [7, 21, 41, 81, 121, 181, 241],
                        'gamma': [0.1, 1, 5, 10, 20, 30, 50]},
  'LabelSpreading': {'n_neighbors': [7, 21, 41, 81, 121, 181, 241],
                    'gamma': [0.1, 1, 5, 10, 20, 30, 50],
                    'alpha': [0.15, 0.2, 0.35, 0.55, 0.75, 0.95]},
  'SGDClassifier': {'loss' : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],
                'penalty' : ['l1', 'l2', 'elasticnet'],
                'alpha' : np.logspace(-4, 4, 25),
                'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],
                'class_weight' : [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}],
                'eta0' : [1, 10, 100]},
  'LinearSVC': {'penalty': ['l1', 'l2'],
                'loss': ['hinge', 'squared_hinge'],
                'class_weight': [None, 'balanced']},
  'stack' : {'meta_classifier__C': np.logspace(-4, 4, 3),
            'meta_classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
            'meta_classifier__degree': range(2,4),
            'meta_classifier__gamma': np.logspace(-3, 1, 3)
            }
}


models_base = [
        ('LogisticRegression', LogisticRegression()),
        ('Ridge', RidgeClassifier()),
        ('DecisionTree', DecisionTreeClassifier()),
        ('K-nn', KNeighborsClassifier()),
        ('RandomForest', RandomForestClassifier()),
        ('BernoulliNB', BernoulliNB()),
        ('GaussianNB', GaussianNB()),
]

# Define a function for dimensionality reduction using PCA
def dimensionality_reduction(X_train, X_test, n_components):
    X_train.columns = X_train.columns.astype(str)
    X_test.columns = X_test.columns.astype(str)
    pca = PCA(n_components=n_components)
    X_train_pca = pca.fit_transform(X_train)
    X_test_pca = pca.transform(X_test)

    #save model
    pickle.dump(pca, open('../models/pca_'+scaler+tutti_picchi+'npicchi'+str(n)+'_npicchimax'+str(picco_max)+'.pkl',"wb"))

    X_train_pca = pd.DataFrame(X_train_pca)
    X_test_pca = pd.DataFrame(X_test_pca)
    #print(X_train_pca.shape)
    return X_train_pca, X_test_pca

def makeTuningCluster(model, X, y, name):
    score = {}
    params = param_grid_cluster[name]
    max_score = {}
    max_score['acc'] = 0
    model_best = clone(model)
    for param in params:
        #print(param)
        model_cl = clone(model)
        model_cl.set_params(**param)
        #print(model.get_params())
        model_cl.fit(X)
        y_pred = model_cl.labels_
        score = makeScore(y,y_pred)
        #print(score['acc'])
        if score['acc'] > max_score['acc']:
            max_score = score
            model_best = clone(model_cl)
            y_pred_max = y_pred
            '''print('Max:')
            print(max_score)
            print(model_best.get_params())'''

    score_cluster = makeCrossValidationCluster(model_best, X)
    '''print('Model best final:')
    print(model_best.get_params())'''
    return clone(model_best), max_score, score_cluster,y_pred_max