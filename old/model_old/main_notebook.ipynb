{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "from scipy import stats\n",
    "from pca import pca\n",
    "from IPython.display import display\n",
    "\n",
    "from src.visualization import feature_importances_plot\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "    \n",
    "# warnings -> to silence warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_JOBS = -1\n",
    "class_names = [\"Canis\", \"Dysg. Equisimilis\", \"Dysg. Dysgalactiae\"]\n",
    "\n",
    "map_target = {\n",
    "    \"Streptococcus canis\": 0,\n",
    "    \"Streptococcus dysgalactiae subsp. equisimilis\": 1,\n",
    "    \"Streptococcus dysgalactiae subsp. dysgalactiae\": 2\n",
    "}\n",
    "\n",
    "map_target_inv = {\n",
    "    0: \"Strept. canis\",\n",
    "    1: \"Strept. dysg. equisimilis\",\n",
    "    2: \"Strept. dysg. dysgalactiae\"\n",
    "}\n",
    "\n",
    "df_46 = pd.read_csv(\"data/Dati_Matemaldomics_46picchi.csv\",\n",
    "                delimiter=';', index_col='ID Strain')\n",
    "df_306 = pd.read_csv(\"data/Dati_Matemaldomics_306picchi.csv\",\n",
    "                delimiter=';', index_col='ID Strain')\n",
    "\n",
    "maldi_46 = df_46[df_46.columns[9:55]]\n",
    "df_46['target'] = df_46[\"Putative Subspecies\"].map(map_target)\n",
    "\n",
    "maldi_306 = df_306[df_306.columns[9:315]]\n",
    "df_306['target'] = df_306[\"Putative Subspecies\"].map(map_target)\n",
    "\n",
    "maldi_46.fillna(0, inplace=True)\n",
    "maldi_46 = maldi_46.replace(',', '.', regex=True)\n",
    "columns = maldi_46.columns\n",
    "for column in columns:\n",
    "    maldi_46[column] = maldi_46[column].astype(float)\n",
    "display(maldi_46)\n",
    "\n",
    "maldi_306.fillna(0, inplace=True)\n",
    "maldi_306 = maldi_306.replace(',', '.', regex=True)\n",
    "columns = maldi_306.columns\n",
    "for column in columns:\n",
    "    maldi_306[column] = maldi_306[column].astype(float)\n",
    "display(maldi_306)\n",
    "\n",
    "dataframes = [['maldi_46' , maldi_46 , df_46['target']],['maldi_306', maldi_306, df_306['target']]]\n",
    "scaler = MinMaxScaler()\n",
    "models = {'Logistic Regression' : LogisticRegression(random_state=RANDOM_STATE, solver='lbfgs', max_iter=1000),\n",
    "          'Decision Trees' : DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "          'K-nn' : KNeighborsClassifier(), \n",
    "          'Random Forest' : RandomForestClassifier(oob_score=False, n_jobs=N_JOBS, random_state=RANDOM_STATE)}\n",
    "type_features = [\"all_features\", \"pca\"]\n",
    "cv_scores = list()\n",
    "ac_scores = list()\n",
    "cv_scores_base = list()\n",
    "ac_scores_base = list()\n",
    "data_cv = list()\n",
    "data_ac = list()\n",
    "data_cv_base = list()\n",
    "data_ac_base = list()\n",
    "\n",
    "y = df_46['target']\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "x = 0\n",
    "t = 0\n",
    "for type_feature in type_features:\n",
    "    cv_scores.append(list())\n",
    "    ac_scores.append(list())\n",
    "    cv_scores_base.append(list())\n",
    "    ac_scores_base.append(list())\n",
    "    d=0\n",
    "    #for str_df, X in dataframes.items():\n",
    "    for dataframe in dataframes:\n",
    "        str_df = dataframe[0]\n",
    "        X = dataframe[1]\n",
    "        y = dataframe[2]\n",
    "        cv_scores[t].append(list())\n",
    "        ac_scores[t].append(list())\n",
    "        cv_scores_base[t].append(list())\n",
    "        ac_scores_base[t].append(list())\n",
    "        s=0\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        if type_feature == \"pca\":\n",
    "            model_pca = PCA(n_components = 0.95)\n",
    "            X_reduced = model_pca.fit_transform(X_scaled)\n",
    "            print(\"Feature con PCA:\", X_train.shape)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        \n",
    "        data_cv.append(list())\n",
    "        data_ac.append(list())\n",
    "        data_cv_base.append(\"Base_\"+str_df+\"_\"+type_feature)\n",
    "        data_cv[x].append(\"Best_\"+str_df+\"_\"+type_feature)\n",
    "        data_ac[x].append(\"Best_\"+str_df+\"_\"+type_feature)\n",
    "        m=0\n",
    "        for str_model, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            cv_scores_base[t][d].append(cross_val_score(estimator=model, X=X_train, y=y_train,\n",
    "                                                    scoring=\"accuracy\", cv=skfold, n_jobs=N_JOBS, verbose=1))\n",
    "            print(str_model+\" Base\"+\n",
    "                \"\\nDataframe: \"+str_df+\n",
    "                \"\\nFeatures: \"+type_feature)\n",
    "            print(model.get_params())\n",
    "            print(f\"Mean CV accuracy: {cv_scores_base[t][d][m].mean():.4f} +/- {cv_scores_base[t][d][m].std():.4f}\")\n",
    "            print(cv_scores_base[t][d][m])\n",
    "\n",
    "            '''\n",
    "            y_pred = model.predict(X_test)\n",
    "            ac_scores[t][d][s].append(model.score(X_test, y_test))\n",
    "            report = classification_report(y_true=y_test, y_pred=y_pred)\n",
    "            print(report)\n",
    "            '''\n",
    "            if str_model == 'Logistic Regression':\n",
    "                model = LogisticRegression(random_state=RANDOM_STATE, solver='lbfgs', max_iter=1000)\n",
    "                params = {\n",
    "                    \"penalty\": [\"l2\", \"l1\"],\n",
    "                    \"C\": stats.loguniform(1e0, 1e2),\n",
    "                    \"class_weight\": [None, \"balanced\"]\n",
    "                }\n",
    "            if str_model == 'Decision Trees':\n",
    "                print('Altezza albero base'+str(model.get_depth()))\n",
    "                model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "                path = model.cost_complexity_pruning_path(X=X_train, y=y_train)\n",
    "                ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "                \n",
    "                params = {\n",
    "                    'ccp_alpha': ccp_alphas,\n",
    "                    'max_depth': [None,3,5,7,10,15],\n",
    "                    'min_samples_leaf': [1,3,5,10,15,20],\n",
    "                    'min_samples_split': [2,4,6,8,10,12,14,16,18,20],\n",
    "                    'criterion': ['gini','entropy']\n",
    "                }\n",
    "            if str_model == 'K-nn':\n",
    "                model = KNeighborsClassifier()\n",
    "                params = {\n",
    "                    'n_neighbors' : [5,7,9,11,13,15],\n",
    "                    'weights' : ['uniform','distance'],\n",
    "                    'metric' : ['minkowski','euclidean','manhattan']\n",
    "                }\n",
    "            if str_model == 'Random Forest':\n",
    "                model = RandomForestClassifier(oob_score=True, n_jobs=N_JOBS, random_state=RANDOM_STATE)\n",
    "                params = {\n",
    "                    \"n_estimators\": [25, 50, 100, 200, 250, 500],\n",
    "                    \"criterion\": [\"gini\", \"entropy\"],\n",
    "                    \"max_depth\": [None, 1, 2, 5, 10, 20],\n",
    "                    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "                    \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "                }\n",
    "            rs = RandomizedSearchCV(estimator=model, param_distributions=params,\n",
    "                                scoring=\"accuracy\", n_jobs=-1, cv=skfold, verbose=1)\n",
    "            rs.fit(X_train, y_train)\n",
    "            print(str_model+\" Best \\\n",
    "                \\nDataframe: \"+str_df+\n",
    "                \"\\nFeatures: \"+type_feature)\n",
    "            print(rs.best_params_)\n",
    "            print('Random '+str_model+': '+str(rs.best_score_))\n",
    "            parametri = rs.best_params_\n",
    "            model.set_params(**parametri)\n",
    "            print(model.get_params())\n",
    "            model.fit(X_train, y_train)\n",
    "            cv_scores[t][d].append(cross_val_score(estimator=model, X=X_train, y=y_train,\n",
    "                                                    scoring=\"accuracy\", cv=skfold, n_jobs=N_JOBS, verbose=1))\n",
    "            print(f\"Mean CV accuracy: {cv_scores[t][d][m].mean():.4f} +/- {cv_scores[t][d][m].std():.4f}\")\n",
    "            print(cv_scores[t][d][m])\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(8, 5))\n",
    "            plot_confusion_matrix(estimator=model, X=X_test, y_true=y_test,\n",
    "                                cmap='Blues', display_labels=class_names, ax=ax)\n",
    "            plt.title(str_model+\" with best params\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            y_pred = model.predict(X_test)\n",
    "            ac_scores[t][d].append(model.score(X_test, y_test))\n",
    "            report = classification_report(y_true=y_test, y_pred=y_pred)\n",
    "            print(report)\n",
    "\n",
    "            if str_model != 'K-nn' and str_model != 'Random Forest':\n",
    "                #Misura il learning curve del nuovo modello\n",
    "                train_sizes = np.linspace(0.01, 1, 20)\n",
    "                train_sizes, train_scores, valid_scores = learning_curve(estimator=model, X=X_train, y=y_train,\n",
    "                                                                        train_sizes=train_sizes, cv=5,\n",
    "                                                                        scoring=\"accuracy\", n_jobs=-1, shuffle=True, random_state=42)\n",
    "\n",
    "                train_mean = train_scores.mean(axis=1)\n",
    "                valid_mean = valid_scores.mean(axis=1)\n",
    "\n",
    "            #plt.style.use('seaborn')\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(train_sizes, train_mean, label='Training error')\n",
    "            plt.plot(train_sizes, valid_mean, label='Validation error')\n",
    "            plt.ylabel('MSE', fontsize=14)\n",
    "            plt.xlabel('Training set size', fontsize=14)\n",
    "            plt.title('Learning curves for '+str_model+' with best parameter',\n",
    "                    fontsize=18, y=1.03)\n",
    "            plt.ylim(0, 2)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            if str_model == 'Logistic Regression' and type_feature != 'pca':\n",
    "                alphas = np.logspace(-5, 0, 100)\n",
    "                coefs_ridge = []\n",
    "                metrics_ridge = []\n",
    "                coefs_lasso = []\n",
    "                metrics_lasso = []\n",
    "                    \n",
    "                #Rifa ridge e lasso ma questa volta direttamente su le feature, senza polynomial features\n",
    "                for alpha in alphas:\n",
    "                    ridge = Ridge(alpha=alpha)\n",
    "                    ridge.fit(X_train, y_train)\n",
    "                    coefs_ridge.append(ridge.coef_)\n",
    "                    metrics_ridge.append({\"lambda\": alpha, \"r2_train\": ridge.score(X_train, y_train), \"r2_test\": ridge.score(X_test, y_test)})\n",
    "                    lasso = Lasso(alpha=alpha)\n",
    "                    lasso.fit(X_train, y_train)\n",
    "                    coefs_lasso.append(lasso.coef_)\n",
    "                    metrics_lasso.append({\"lambda\": alpha, \"r2_train\": lasso.score(X_train, y_train), \"r2_test\": lasso.score(X_test, y_test)})\n",
    "                    \n",
    "                plt.plot(alphas, coefs_ridge)\n",
    "                plt.xscale(\"log\")\n",
    "                plt.title(\"Ridge without Polynomial Features\")\n",
    "                plt.tight_layout()\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "\n",
    "                plt.plot(alphas, coefs_lasso)\n",
    "                plt.xscale(\"log\")\n",
    "                plt.title(\"Lasso without Polynomial Features\")\n",
    "                plt.tight_layout()\n",
    "                plt.grid()\n",
    "                plt.show()\n",
    "\n",
    "                df_metrics_ridge = pd.DataFrame(metrics_ridge)\n",
    "                df_metrics_ridge[\"difference\"] = df_metrics_ridge[\"r2_train\"] - df_metrics_ridge[\"r2_test\"]\n",
    "\n",
    "                df_metrics_lasso = pd.DataFrame(metrics_lasso)\n",
    "                df_metrics_lasso[\"difference\"] = df_metrics_lasso[\"r2_train\"] - df_metrics_lasso[\"r2_test\"]\n",
    "\n",
    "                sns.lineplot(data=df_metrics_ridge, x=\"lambda\", y=\"r2_train\", label=\"train scores ridge\")\n",
    "                sns.lineplot(data=df_metrics_ridge, x=\"lambda\", y=\"r2_test\", label=\"test scores ridge\")\n",
    "                sns.lineplot(data=df_metrics_lasso, x=\"lambda\", y=\"r2_train\", label=\"train scores lasso\")\n",
    "                sns.lineplot(data=df_metrics_lasso, x=\"lambda\", y=\"r2_test\", label=\"test scores lasso\")\n",
    "                plt.xscale(\"log\")\n",
    "                plt.title(\"Score ridge/lasso without polynomial features\")\n",
    "                plt.grid()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                sns.lineplot(data=df_metrics_ridge, x=\"lambda\", y=\"difference\", label=\"difference scores ridge\")\n",
    "                sns.lineplot(data=df_metrics_lasso, x=\"lambda\", y=\"difference\", label=\"difference scores lasso\")\n",
    "                plt.xscale(\"log\")\n",
    "                plt.title(\"Difference between train and test score for ridge/lasso without polynomial features\")\n",
    "                plt.grid()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            if str_model == 'Decision Trees' and type_feature != 'pca':\n",
    "                #Dizionario con importanza feature\n",
    "                feature_importances = model.feature_importances_\n",
    "                feature_index = X.columns\n",
    "                myDict = dict(zip(feature_index, feature_importances))\n",
    "                myDict = dict(sorted(myDict.items(), key=lambda item: item[1], reverse = False))\n",
    "\n",
    "                #Plot delle 15 feature con pi√π importanza\n",
    "                series = pd.Series(data=myDict.values(), index=myDict.keys()).tail(10)\n",
    "                series.plot(kind=\"barh\", figsize=(8, 5), title=f\"Feature importances for Decision Tree\", legend=None)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                feature_names = feature_index\n",
    "\n",
    "                plt.figure(figsize=(12, 12))\n",
    "                plot_tree(decision_tree=model, \n",
    "                        feature_names=feature_names, \n",
    "                        class_names=class_names, \n",
    "                        filled=True, fontsize=8)\n",
    "                plt.title(\"Decision Tree\")\n",
    "                plt.show()\n",
    "            if str_model == 'K-nn' and type_feature != 'pca':\n",
    "                knn_scores = []\n",
    "                for k in range(1, X_train.shape[0], 2):\n",
    "                    # model definition and training\n",
    "                    knn =  KNeighborsClassifier(n_neighbors=k)\n",
    "                    knn.fit(X=X_train, y=y_train)\n",
    "                    # compute accuracy on test set\n",
    "                    accuracy = knn.score(X_test, y_test)\n",
    "                    # store the results on a list of dictionaries\n",
    "                    metrics = {\"# neighbors\": k, \"accuracy\": accuracy}\n",
    "                    knn_scores.append(metrics)\n",
    "\n",
    "                # convert the list of dictionaries to pandas dataframe\n",
    "                df_knn_scores = pd.DataFrame(data=knn_scores)    #molto facile eveloce da dizionario a dataframe \n",
    "                display(df_knn_scores)\n",
    "\n",
    "                mask = df_knn_scores[\"accuracy\"] == df_knn_scores[\"accuracy\"].max()\n",
    "                knn_k = df_knn_scores['accuracy'].idxmax()\n",
    "                n = df_knn_scores['# neighbors'][knn_k]\n",
    "\n",
    "                plt.figure(figsize=(7, 5))\n",
    "                plt.title(\"KNN accuracy as function of the number of neighbors\")\n",
    "                sns.lineplot(x=\"# neighbors\", y=\"accuracy\", data=df_knn_scores)\n",
    "                plt.grid(linestyle='-.', linewidth=0.5)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                display(df_knn_scores[mask])\n",
    "                \n",
    "            if str_model == 'Random Forest' and type_feature != 'pca':\n",
    "                feature_importances_plot(model=model, labels=X.columns)\n",
    "                print(\"Score Random Forest base: \", model.oob_score_)\n",
    "\n",
    "                r2_rf = r2_score(y_test, y_pred)\n",
    "                print('R2 score of random forest classifier on test set: {:.3f}'.format(r2_rf))\n",
    "\n",
    "                rmse_rf = mean_squared_error(y_test, y_pred, squared=False)\n",
    "                print('Mean squared error of random forest classifier on test set: {:.3f}'.format(rmse_rf))\n",
    "                \n",
    "            plt.figure(figsize=(6, 4))\n",
    "            plt.title('CV '+str_model+' model scores')\n",
    "            plt.plot(cv_scores_base[t][d][m], marker=\"o\")\n",
    "            plt.plot(cv_scores[t][d][m], marker=\"o\")\n",
    "            plt.xticks(ticks=range(1, 5))\n",
    "            plt.xlabel(\"fold\")\n",
    "            plt.ylabel(\"accuracy\")\n",
    "            plt.legend([\"Base model\", \"Best model\"])\n",
    "            plt.tight_layout()\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "            data_cv[x].append(str(cv_scores[t][d][m].mean()))\n",
    "            data_ac[x].append(str(ac_scores[t][d][m]))\n",
    "            m += 1\n",
    "        x += 1           \n",
    "        d += 1\n",
    "    t += 1\n",
    "\n",
    "feature_index = ['Feature','Logistic Regression', 'Decision Tree', 'Knn', 'Random Forest']\n",
    "df_ac_best = pd.DataFrame(data_ac, columns=feature_index)\n",
    "df_ac_best.to_csv('accuracy_maldi.csv', index=False, mode='w')\n",
    "df_cv_best = pd.DataFrame(data_cv, columns=feature_index)\n",
    "df_cv_best.to_csv('cv_maldi.csv', index=False, mode='w')\n",
    "\n",
    "    \n",
    "print(\"AC SCORES BEST\")\n",
    "display(df_ac_best)\n",
    "print(\"CV SCORES BEST\")\n",
    "display(df_cv_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dabbc12efdf8f4ed23c27b017df9932148986fc5640203004a39cc19ec89775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
